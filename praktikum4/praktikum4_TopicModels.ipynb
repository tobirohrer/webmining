{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import fastparquet\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import _pickle as pickle\n",
    "\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "random.seed(1234)\n",
    "\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Topic Model on CommonCrawl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Loading and Filtering the Data\n",
    "There are 28 files with ~2500 records each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.1 s, sys: 12.8 s, total: 37.9 s\n",
      "Wall time: 39.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load the parquet files into a single dataframe\n",
    "\n",
    "result = pd.DataFrame()\n",
    "#for filename in os.listdir(\"C:\\\\tmp\\\\my.tar\\\\istjoscha_cc_warc_wat_wet_acht_lang\"):\n",
    "for pth in sorted(glob.glob(\"data/CC/*\")):\n",
    "    ds = pd.read_parquet(pth)\n",
    "    #print(pth)\n",
    "    #print(ds.shape)\n",
    "    result = result.append(ds)\n",
    "    #print(result.shape)\n",
    "\n",
    "#In case you are running out of memory, try to run the filter in the following paragraph on ds before appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter on English and university-related content\n",
    "result = result.loc[(result['Guessed-Language']=='en') & (result['Plaintext'].str.contains(\"niversity\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Constructing the Corpus and Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary function for \"straight-forward\" natural language preprocessing\n",
    "\n",
    "def cleanup_text(record):\n",
    "    text = record['Plaintext']\n",
    "    # Remove newlines\n",
    "    text = text.replace(r'\\n', ' ')\n",
    "    words = text.split()\n",
    "\n",
    "    # Default list of Stopwords\n",
    "    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any',\n",
    "                      u'are', u'arent', u'as', u'at',\n",
    "                      u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by',\n",
    "                      u'can', 'cant', 'come', u'could', 'couldnt',\n",
    "                      u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during',\n",
    "                      u'each',\n",
    "                      u'few', 'finally', u'for', u'from', u'further',\n",
    "                      u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here',\n",
    "                      u'hers', u'herself', u'him', u'himself', u'his', u'how',\n",
    "                      u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself',\n",
    "                      u'just',\n",
    "                      u'll',\n",
    "                      u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself',\n",
    "                      u'no', u'nor', u'not', u'now',\n",
    "                      u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves',\n",
    "                      u'out', u'over', u'own',\n",
    "                      u'r', u're',\n",
    "                      u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such',\n",
    "                      u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then',\n",
    "                      u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too',\n",
    "                      u'under', u'until', u'up',\n",
    "                      u'very',\n",
    "                      u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while',\n",
    "                      u'who', u'whom', u'why', u'will', u'with', u'wont', u'would',\n",
    "                      u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n",
    "\n",
    "    # Custom List of Stopwords - Add your own here\n",
    "    stopwords_custom = ['']\n",
    "    stopwords = stopwords_core + stopwords_custom\n",
    "    stopwords = [word.lower() for word in stopwords]\n",
    "\n",
    "    text_out = [re.sub('[^a-zA-Z0-9]', '', word) for word in words]  # Remove special characters\n",
    "    text_out = [word.lower() for word in text_out if\n",
    "                len(word) > 2 and word.lower() not in stopwords]  # Remove stopwords and words under X length\n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 242 ms, total: 19.9 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#build gensim corpus\n",
    "\n",
    "result=result.reset_index()\n",
    "texts=[]\n",
    "for index, row in result.iterrows():\n",
    "    texts.append(cleanup_text(row))\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "dictionary.compactify()\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 21s, sys: 962 ms, total: 3min 22s\n",
      "Wall time: 34.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ldamodelnormal = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=10, chunksize=100, update_every=0, alpha=1/15, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Inspecting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to create descriptive tables (doc-topic probabilities) and visualizations for LDA models \n",
    "def getModelResults(ldamodel, corpus, dictionary):\n",
    "    vis = pyLDAvis.gensim.prepare(ldamodel,corpus, dictionary, sort_topics=False)\n",
    "    transformed = ldamodel.get_document_topics(corpus)\n",
    "    df = pd.DataFrame.from_records([{v:k for v, k in row} for row in transformed])\n",
    "    return vis, df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top topic per document into a list\n",
    "def maxTop(x):\n",
    "    mx = max(x,key=lambda item:item[1])\n",
    "    if (mx[1]>0.0):\n",
    "        return(mx[0])\n",
    "    else:\n",
    "        return 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.9 s, sys: 1.02 s, total: 41.9 s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get descriptive stuff for all models\n",
    "normalv, dfnormal = getModelResults(ldamodelnormal, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"source\" + 0.005*\"research\" + 0.005*\"data\" + 0.004*\"care\" + 0.004*\"college\" + 0.004*\"authors\" + 0.004*\"presentation\" + 0.004*\"journal\"'),\n",
       " (1,\n",
       "  '0.015*\"download\" + 0.005*\"thin\" + 0.004*\"films\" + 0.004*\"magnetic\" + 0.003*\"research\" + 0.003*\"book\" + 0.003*\"film\" + 0.003*\"international\"'),\n",
       " (2,\n",
       "  '0.009*\"june\" + 0.009*\"july\" + 0.009*\"2017\" + 0.007*\"february\" + 0.007*\"january\" + 0.007*\"march\" + 0.007*\"october\" + 0.007*\"september\"'),\n",
       " (3,\n",
       "  '0.004*\"people\" + 0.003*\"business\" + 0.003*\"best\" + 0.003*\"life\" + 0.002*\"free\" + 0.002*\"make\" + 0.002*\"day\" + 0.002*\"back\"'),\n",
       " (4,\n",
       "  '0.010*\"school\" + 0.010*\"college\" + 0.008*\"ago\" + 0.005*\"state\" + 0.004*\"high\" + 0.004*\"elementary\" + 0.004*\"years\" + 0.003*\"day\"'),\n",
       " (5,\n",
       "  '0.003*\"free\" + 0.003*\"design\" + 0.003*\"data\" + 0.002*\"share\" + 0.002*\"2017\" + 0.002*\"years\" + 0.002*\"dad\" + 0.002*\"download\"'),\n",
       " (6,\n",
       "  '0.036*\"architects\" + 0.012*\"health\" + 0.009*\"public\" + 0.007*\"name\" + 0.006*\"english\" + 0.004*\"form\" + 0.004*\"board\" + 0.003*\"city\"'),\n",
       " (7,\n",
       "  '0.010*\"law\" + 0.009*\"required\" + 0.008*\"years\" + 0.007*\"lawyer\" + 0.005*\"view\" + 0.004*\"sex\" + 0.004*\"legal\" + 0.003*\"big\"'),\n",
       " (8,\n",
       "  '0.008*\"south\" + 0.007*\"north\" + 0.006*\"east\" + 0.005*\"west\" + 0.003*\"islands\" + 0.003*\"republic\" + 0.002*\"iran\" + 0.002*\"central\"'),\n",
       " (9,\n",
       "  '0.006*\"system\" + 0.003*\"nuclear\" + 0.003*\"exchange\" + 0.003*\"control\" + 0.003*\"data\" + 0.003*\"study\" + 0.003*\"using\" + 0.003*\"results\"'),\n",
       " (10,\n",
       "  '0.009*\"jun\" + 0.009*\"mar\" + 0.008*\"jan\" + 0.008*\"apr\" + 0.008*\"oct\" + 0.008*\"sep\" + 0.008*\"nov\" + 0.008*\"jul\"'),\n",
       " (11,\n",
       "  '0.015*\"project\" + 0.015*\"report\" + 0.007*\"water\" + 0.007*\"energy\" + 0.004*\"technology\" + 0.003*\"research\" + 0.003*\"waste\" + 0.003*\"science\"'),\n",
       " (12,\n",
       "  '0.004*\"words\" + 0.003*\"art\" + 0.003*\"film\" + 0.003*\"john\" + 0.003*\"april\" + 0.003*\"architects\" + 0.002*\"july\" + 0.002*\"toronto\"'),\n",
       " (13,\n",
       "  '0.004*\"2016\" + 0.004*\"health\" + 0.003*\"education\" + 0.003*\"people\" + 0.003*\"public\" + 0.003*\"students\" + 0.003*\"world\" + 0.003*\"says\"'),\n",
       " (14,\n",
       "  '0.059*\"temp\" + 0.032*\"max\" + 0.032*\"min\" + 0.009*\"weight\" + 0.009*\"forecast\" + 0.008*\"loss\" + 0.006*\"slim\" + 0.006*\"estate\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print regular topics (top word probabilities)\n",
    "ldamodelnormal.print_topics(num_words=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil I - Aufgabe 1\n",
    "## Für welche Topics können Sie intuitiv Überbegriffe bilden? Notieren Sie sich diese bzw. legen Sie eine entsprechende „lookup-tabelle“ als Datenstruktur an. Welche Topics erscheinen sinnvoll, welche nicht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    0:'Research', \n",
    "    1: None,\n",
    "    2: 'Month',\n",
    "    3: 'Life',\n",
    "    4: 'Education', \n",
    "    5: 'Medicine',\n",
    "    6: 'Architect', \n",
    "    7: 'Politics', \n",
    "    8: 'Country', \n",
    "    9: 'Education', \n",
    "    10: 'Month', \n",
    "    11: 'Technology',\n",
    "    12: None,\n",
    "    13: 'Government',\n",
    "    14: 'Temperature'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wordcloud for topic: adult content\n",
    "\n",
    "\n",
    "# get the top topic for a known adult content document \n",
    "docTopTopics = [maxTop(x) for x in ldamodelnormal.get_document_topics(corpus)]\n",
    "adultTopicId = docTopTopics[15]\n",
    "\n",
    "\n",
    "#gather most relevant terms for the given topic\n",
    "topics_terms = ldamodelnormal.state.get_lambda()\n",
    "tmpDict = {}\n",
    "for i in range(1, len(topics_terms[0])):\n",
    "    tmpDict[ldamodelnormal.id2word[i]]=topics_terms[adultTopicId,i]\n",
    "\n",
    "\n",
    "# draw the wordcloud\n",
    "wordcloud = WordCloud( margin=0,max_words=20 ).generate_from_frequencies(tmpDict)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n",
    "print(\"Adult Topic Id = \" + str(adultTopicId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect regular topics - ATTENTION: ALL TOPICS ARE SHIFTED WITH ID +1 w.r.t. GENSIM\n",
    "normalv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inspect first 20 documents and their topic distributions\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.concat([result['Target-URI'], dfnormal], axis=1).iloc[0:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Model on NHTSA (Using HANA Text Analysis Index Table)\n",
    "## 2.1  Constructing the Corpus (Using HANA Text Analysis Index Table) and Building the Model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data ingestion and filtering\n",
    "\n",
    "cmpl = pd.read_csv(\"data/NHTSA_HANA/cmpl.csv\", header=None,  keep_default_na=False)\n",
    "ta = pd.read_csv(\"data/NHTSA_HANA/data.csv\", header=None,  keep_default_na=False)\n",
    "\n",
    "ta.columns = [\"CMPLID\", \"TA_RULE\",\"TA_COUNTER\",\"TA_TOKEN\",\"TA_LANGUAGE\",\"TA_TYPE\",\"TA_TYPE_EXPANDED\",\n",
    "              \"TA_NORMALIZED\" ,\"TA_STEM\",\"TA_PARAGRAPH\",\"TA_SENTENCE\",\"TA_CREATED_AT\",\"TA_OFFSET\",\"TA_PARENT\" ]\n",
    "cmpl.columns = [\"CMPLID\", \"ODINO\", \"MFR_NAME\", \"MAKETEXT\", \"MODELTXT\", \"YEARTXT\", \"CRASH\", \"FAILDATE\", \"FIRE\", \"INJURED\", \"DEATHS\", \"COMPDESC\", \"CITY\", \"STATE\", \"VIN\", \"DATEA\", \"LDATE\", \"MILES\", \"OCCURRENCES\", \"CDESCR\", \"CMPL_TYPE\", \"POLICE_RPT_VN\", \"PURCH_DT\", \"ORIG_OWNER_YN\", \"ANTI_BRAKES_YN\", \"CRUISE_CONT_YN\", \"NUM_CYLS\", \"DRIVE_TRAIN\", \"FUEL_SYS\", \"FUEL_TYPE\", \"TRANS_TYPE\", \"VEH_SPEED\", \"DOT\", \"TIRE_SIZE\", \"LOC_OF_TIRE\", \"TIRE_FAIL_TYPE\", \"ORIG_EQUIP_YN\", \"MANUF_DT\", \"SEAT_TYPE\", \"RESTRAINT_TYPE\", \"DEALER_NAME\", \"DEALER_TEL\", \"DEALER_CITY\", \"DEALER_STATE\", \"DEALER_ZIP\", \"PROD_TYPE\", \"REPAIRED_YN\", \"MEDICAL_ATTN\", \"VEHICELS_TOWED_YN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering and aggregation\n",
    "\n",
    "ta = ta[ta.TA_TYPE.isin(['noun', 'adjective']) ]\n",
    "ta.TA_TOKEN = ta.TA_TOKEN.str.lower()\n",
    "ta = ta[~ta.TA_TOKEN.isin(['car', 'vehicle']) ] #use as additional stop words\n",
    "cmpl = cmpl[cmpl.COMPDESC.isin([ 'AIR BAGS','VISIBILITY/WIPER','EXTERIOR LIGHTING','FUEL/PROPULSION SYSTEM', 'SERVICE BRAKES','WHEELS']) ]\n",
    "\n",
    "ta = ta.merge(cmpl.loc[:,'CMPLID'], on=['CMPLID'], how='inner')\n",
    "tagrouped = ta.groupby('CMPLID')['TA_TOKEN'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show category counts\n",
    "relevantComplaints = cmpl[cmpl.CMPLID.isin(tagrouped.index.tolist())].sort_values(by=['CMPLID'])\n",
    "counter=collections.Counter(relevantComplaints.COMPDESC)\n",
    "print(counter)\n",
    "print(\"Anzahl Dokumente: \" + str(len(relevantComplaints)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create gensim data structure\n",
    "nhtsadic = corpora.Dictionary(tagrouped.tolist())\n",
    "nhtsadic.filter_extremes(no_below=10, no_above=0.4)\n",
    "nhtsadic.compactify()\n",
    "nhtsacorpus = [nhtsadic.doc2bow(text) for text in tagrouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#build topic model and create descriptive stuff\n",
    "nhtsalda = LdaMulticore(nhtsacorpus, num_topics=10, id2word = nhtsadic, passes=20, alpha=0.0000001, random_state=1)\n",
    "nhtsavis, dfnhtsa = getModelResults(nhtsalda, nhtsacorpus, nhtsadic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Inspecting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nhtsavis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
