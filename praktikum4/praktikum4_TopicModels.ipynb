{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import fastparquet\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import _pickle as pickle\n",
    "\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "random.seed(1234)\n",
    "\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Topic Model on CommonCrawl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Loading and Filtering the Data\n",
    "There are 28 files with ~2500 records each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Load the parquet files into a single dataframe\n",
    "\n",
    "result = pd.DataFrame()\n",
    "#for filename in os.listdir(\"C:\\\\tmp\\\\my.tar\\\\istjoscha_cc_warc_wat_wet_acht_lang\"):\n",
    "for pth in sorted(glob.glob(\"data/CC/*\")):\n",
    "    ds = pd.read_parquet(pth)\n",
    "    #print(pth)\n",
    "    #print(ds.shape)\n",
    "    result = result.append(ds)\n",
    "    #print(result.shape)\n",
    "\n",
    "#In case you are running out of memory, try to run the filter in the following paragraph on ds before appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter on English and university-related content\n",
    "result = result.loc[(result['Guessed-Language']=='en') & (result['Plaintext'].str.contains(\"niversity\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Constructing the Corpus and Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary function for \"straight-forward\" natural language preprocessing\n",
    "\n",
    "def cleanup_text(record):\n",
    "    text = record['Plaintext']\n",
    "    # Remove newlines\n",
    "    text = text.replace(r'\\n', ' ')\n",
    "    words = text.split()\n",
    "\n",
    "    # Default list of Stopwords\n",
    "    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any',\n",
    "                      u'are', u'arent', u'as', u'at',\n",
    "                      u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by',\n",
    "                      u'can', 'cant', 'come', u'could', 'couldnt',\n",
    "                      u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during',\n",
    "                      u'each',\n",
    "                      u'few', 'finally', u'for', u'from', u'further',\n",
    "                      u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here',\n",
    "                      u'hers', u'herself', u'him', u'himself', u'his', u'how',\n",
    "                      u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself',\n",
    "                      u'just',\n",
    "                      u'll',\n",
    "                      u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself',\n",
    "                      u'no', u'nor', u'not', u'now',\n",
    "                      u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves',\n",
    "                      u'out', u'over', u'own',\n",
    "                      u'r', u're',\n",
    "                      u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such',\n",
    "                      u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then',\n",
    "                      u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too',\n",
    "                      u'under', u'until', u'up',\n",
    "                      u'very',\n",
    "                      u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while',\n",
    "                      u'who', u'whom', u'why', u'will', u'with', u'wont', u'would',\n",
    "                      u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n",
    "\n",
    "    # Custom List of Stopwords - Add your own here\n",
    "    stopwords_custom = ['']\n",
    "    stopwords = stopwords_core + stopwords_custom\n",
    "    stopwords = [word.lower() for word in stopwords]\n",
    "\n",
    "    text_out = [re.sub('[^a-zA-Z0-9]', '', word) for word in words]  # Remove special characters\n",
    "    text_out = [word.lower() for word in text_out if\n",
    "                len(word) > 2 and word.lower() not in stopwords]  # Remove stopwords and words under X length\n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#build gensim corpus\n",
    "\n",
    "result=result.reset_index()\n",
    "texts=[]\n",
    "for index, row in result.iterrows():\n",
    "    texts.append(cleanup_text(row))\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)   # Create a dictionary representation of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4)   # Filter out words that occur less than 10 documents and more than 40% of the documents.\n",
    "dictionary.compactify()   # assign new word ids to all words.\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # Bag-of-words representation of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[1000]   # example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus   # word-id, frequency of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ldamodelnormal = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=10, chunksize=100, update_every=0, alpha=1/15, random_state=1)\n",
    "# hyperparameter: \n",
    "# corpus: word-id, frequency of the word\n",
    "# num_topics: number of topics\n",
    "# id2word: representation of the documents\n",
    "# passes: how many times the algorithm is supposed to pass over the whole corpus\n",
    "# chunksize: number of documents to consider at once (affects the memory consumption)\n",
    "# update_every: update the model every chunks\n",
    "# alpha: smoothing parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Inspecting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to create descriptive tables (doc-topic probabilities) and visualizations for LDA models \n",
    "def getModelResults(ldamodel, corpus, dictionary):\n",
    "    vis = pyLDAvis.gensim.prepare(ldamodel,corpus, dictionary, sort_topics=False)\n",
    "    transformed = ldamodel.get_document_topics(corpus)\n",
    "    df = pd.DataFrame.from_records([{v:k for v, k in row} for row in transformed])\n",
    "    return vis, df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top topic per document into a list\n",
    "def maxTop(x):\n",
    "    mx = max(x,key=lambda item:item[1])\n",
    "    if (mx[1]>0.0):\n",
    "        return(mx[0])\n",
    "    else:\n",
    "        return 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get descriptive stuff for all models\n",
    "normalv, dfnormal = getModelResults(ldamodelnormal, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print regular topics (top word probabilities)\n",
    "ldamodelnormal.print_topics(num_words=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Teil 1</span>\n",
    "## <span style=\"color:green\">Aufgabe 1: Schauen Sie sich die Topic-Wortverteilungen des erstellten Modells an (in Textform, der interaktiven Ausgabe, oder als Wordcloud). Für welche Topics können Sie intuitiv Überbegriffe bilden? Notieren Sie sich diese bzw. legen Sie eine entsprechende „lookup-tabelle“ als Datenstruktur an. Welche Topics erscheinen sinnvoll, welche nicht?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    0: 'Research', \n",
    "    1: 'Misc',\n",
    "    2: 'Month',\n",
    "    3: 'Life',\n",
    "    4: 'Education', \n",
    "    5: 'Misc',\n",
    "    6: 'Medicine', \n",
    "    7: 'Adult', \n",
    "    8: 'Country', \n",
    "    9: 'Misc', \n",
    "    10: 'Month', \n",
    "    11: 'Technology',\n",
    "    12: 'Misc',\n",
    "    13: 'Government',\n",
    "    14: 'Climate'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wordcloud for topic: adult content\n",
    "\n",
    "# get the top topic for a known adult content document \n",
    "docTopTopics = [maxTop(x) for x in ldamodelnormal.get_document_topics(corpus)]\n",
    "adultTopicId = docTopTopics[15]\n",
    "\n",
    "#gather most relevant terms for the given topic\n",
    "topics_terms = ldamodelnormal.state.get_lambda()\n",
    "tmpDict = {}\n",
    "for i in range(1, len(topics_terms[0])):\n",
    "    tmpDict[ldamodelnormal.id2word[i]]=topics_terms[adultTopicId,i]\n",
    "\n",
    "# draw the wordcloud\n",
    "wordcloud = WordCloud( margin=0,max_words=20 ).generate_from_frequencies(tmpDict)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n",
    "print(\"Adult Topic Id = \" + str(adultTopicId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect regular topics - ATTENTION: ALL TOPICS ARE SHIFTED WITH ID +1 w.r.t. GENSIM\n",
    "normalv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inspect first 20 documents and their topic distributions\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.concat([result['Target-URI'], dfnormal], axis=1).iloc[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Teil 1</span>\n",
    "## <span style=\"color:green\">Aufgabe 2: Notieren Sie sich, welches Topic in Codeblock 11 als „adult content“ identifiziert wurde. Filtern Sie für die weiteren Aufgaben die entsprechenden Records aus dem „result“ DataFrame aus, also z.B. alle Dokumente mit einer entsprechenden Topicwahrscheinlichkeit > 50%. Öffnen Sie nicht die Links zu den entsprechenden Dokumenten im Browser. Aktivieren Sie sicherheitshalber den installierten Browser-Filter.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnormal = dfnormal.sort_index(axis=1)\n",
    "result_new = pd.concat([result['Target-URI'], dfnormal], axis=1)\n",
    "result_new[result_new[7] > 0.5][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Aufgabe 3: Schauen Sie sich nun für einige andere Topics stichprobenartig Dokumente an. Passen diese zu den vorher von Ihnen vergebenen Topic-Überbegriffen? Warum bzw. warum nicht?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(topic_id, topic_prob):\n",
    "    print(\"Topic #\" + str(topic_id) + \": \" + topics[topic_id])\n",
    "    display(result_new[result_new[topic_id] > topic_prob][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topics)):\n",
    "    if (i != 7):\n",
    "        print_topics(i, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Aufgabe 4: Formulieren Sie Anfragen zu bestimmten Topic-Mischungen (z.B. Topic A > 40% und Topic B > 40%). Passen die gematchten Dokumente zu Ihren Erwartungen? Warum bzw. warum nicht?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics[0], \" AND \", topics[4])\n",
    "result_new[(result_new[0] > 0.4) &  (result_new[4] > 0.4)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics[6], \" AND \", topics[11])\n",
    "result_new[(result_new[6] > 0.4) &  (result_new[11] > 0.4)][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Aufgabe 5: Berechnen Sie zwei neue Modelle (auf dem Original-Corpus mit Adult-Content) mit verändertem Glättungsparameter für die Dokument-Topic Zuordnungen. Die restlichen Parameter sollen beibehalten werden. Berechnen Sie ein Modell mit Glättungsparameter=1 und ein Modell mit Glättungsparameter=10^-18. Schauen Sie sich wieder jeweils die ersten 20 Zeilen der Dokument-Topic Wahrscheinlichkeitsmatrizen an. Plotten Sie weiterhin die Häufigkeitsverteilungen der „Nicht-NaN-Topics“ pro Dokument.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_notna_values(df_notna, alpha):\n",
    "    df_notna= dfnormal.notna().sum() \n",
    "    df_notna = pd.DataFrame(df_notna)\n",
    "    df_notna.reset_index(inplace=True)\n",
    "    df_notna.columns = [\"topic\", \"number\"]\n",
    "    sns.set(rc={'figure.figsize':(20,7)})\n",
    "    ax = sns.barplot(x=\"topic\", y=\"number\",data=df_notna).set_title('notna-frequency')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig('plots/barplot_alpha_' + alpha + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Glättungsparameter (alpha) = 1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodelnormal = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=10, chunksize=100, update_every=0, alpha=1, random_state=1)\n",
    "normalv, dfnormal = getModelResults(ldamodelnormal, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnormal = dfnormal.sort_index(axis=1)\n",
    "dfnormal.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_notna_values(dfnormal, '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Glättungsparameter (alpha) = 10<sup>-18</sup></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodelnormal = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=10, chunksize=100, update_every=0, alpha=10**(-18), random_state=1)\n",
    "normalv, dfnormal = getModelResults(ldamodelnormal, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnormal = dfnormal.sort_index(axis=1)\n",
    "dfnormal.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_notna_values(dfnormal, '10_-18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Model on NHTSA (Using HANA Text Analysis Index Table)\n",
    "## 2.1  Constructing the Corpus (Using HANA Text Analysis Index Table) and Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data ingestion and filtering\n",
    "\n",
    "cmpl = pd.read_csv(\"data/NHTSA_HANA/cmpl.csv\", header=None,  keep_default_na=False)\n",
    "ta = pd.read_csv(\"data/NHTSA_HANA/data.csv\", header=None,  keep_default_na=False)\n",
    "\n",
    "ta.columns = [\"CMPLID\", \"TA_RULE\",\"TA_COUNTER\",\"TA_TOKEN\",\"TA_LANGUAGE\",\"TA_TYPE\",\"TA_TYPE_EXPANDED\",\n",
    "              \"TA_NORMALIZED\" ,\"TA_STEM\",\"TA_PARAGRAPH\",\"TA_SENTENCE\",\"TA_CREATED_AT\",\"TA_OFFSET\",\"TA_PARENT\" ]\n",
    "cmpl.columns = [\"CMPLID\", \"ODINO\", \"MFR_NAME\", \"MAKETEXT\", \"MODELTXT\", \"YEARTXT\", \"CRASH\", \"FAILDATE\", \"FIRE\", \"INJURED\", \"DEATHS\", \"COMPDESC\", \"CITY\", \"STATE\", \"VIN\", \"DATEA\", \"LDATE\", \"MILES\", \"OCCURRENCES\", \"CDESCR\", \"CMPL_TYPE\", \"POLICE_RPT_VN\", \"PURCH_DT\", \"ORIG_OWNER_YN\", \"ANTI_BRAKES_YN\", \"CRUISE_CONT_YN\", \"NUM_CYLS\", \"DRIVE_TRAIN\", \"FUEL_SYS\", \"FUEL_TYPE\", \"TRANS_TYPE\", \"VEH_SPEED\", \"DOT\", \"TIRE_SIZE\", \"LOC_OF_TIRE\", \"TIRE_FAIL_TYPE\", \"ORIG_EQUIP_YN\", \"MANUF_DT\", \"SEAT_TYPE\", \"RESTRAINT_TYPE\", \"DEALER_NAME\", \"DEALER_TEL\", \"DEALER_CITY\", \"DEALER_STATE\", \"DEALER_ZIP\", \"PROD_TYPE\", \"REPAIRED_YN\", \"MEDICAL_ATTN\", \"VEHICELS_TOWED_YN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering and aggregation\n",
    "\n",
    "ta = ta[ta.TA_TYPE.isin(['noun', 'adjective']) ]\n",
    "ta.TA_TOKEN = ta.TA_TOKEN.str.lower()\n",
    "ta = ta[~ta.TA_TOKEN.isin(['car', 'vehicle']) ] #use as additional stop words\n",
    "cmpl = cmpl[cmpl.COMPDESC.isin([ 'AIR BAGS','VISIBILITY/WIPER','EXTERIOR LIGHTING','FUEL/PROPULSION SYSTEM', 'SERVICE BRAKES','WHEELS']) ]\n",
    "\n",
    "ta = ta.merge(cmpl.loc[:,'CMPLID'], on=['CMPLID'], how='inner')\n",
    "tagrouped = ta.groupby('CMPLID')['TA_TOKEN'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show category counts\n",
    "relevantComplaints = cmpl[cmpl.CMPLID.isin(tagrouped.index.tolist())].sort_values(by=['CMPLID'])\n",
    "counter=collections.Counter(relevantComplaints.COMPDESC)\n",
    "print(counter)\n",
    "print(\"Anzahl Dokumente: \" + str(len(relevantComplaints)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create gensim data structure\n",
    "nhtsadic = corpora.Dictionary(tagrouped.tolist())\n",
    "nhtsadic.filter_extremes(no_below=10, no_above=0.4)\n",
    "nhtsadic.compactify()\n",
    "nhtsacorpus = [nhtsadic.doc2bow(text) for text in tagrouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#build topic model and create descriptive stuff\n",
    "nhtsalda = LdaMulticore(nhtsacorpus, num_topics=10, id2word = nhtsadic, passes=20, alpha=0.0000001, random_state=1)\n",
    "nhtsavis, dfnhtsa = getModelResults(nhtsalda, nhtsacorpus, nhtsadic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Inspecting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nhtsavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Teil 2</span>\n",
    "## <span style=\"color:green\">Aufgabe 1: Machen Sie sich, ähnlich wie in Teil I, mit dem erstellten Topicmodell für den NHTSA-Teildatensatz vertraut, indem Sie einzelne Topics bzw. Dokumente genauer unter die Lupe nehmen. Dokumentieren Sie Ihre Erkenntnisse, d.h. in welchen Fällen Sie das Modell für sinnvoll halten und in welchen nicht.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    0: 'Headlight', \n",
    "    1: 'Wheel',\n",
    "    2: 'Brake',\n",
    "    3: 'Recall',\n",
    "    4: 'Light', \n",
    "    5: 'Engine',\n",
    "    6: 'Fuel', \n",
    "    7: 'Tire', \n",
    "    8: 'Windshield', \n",
    "    9: 'Contact'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhtsalda.print_topics(num_words=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Aufgabe 2: Überführen Sie das Soft-Clustering in ein Hard-Clustering, indem Sie einen Vektor erstellen, der pro Dokument das Topic mit der höchsten Wahrscheinlichkeit enthält. Die NHTSA-Kategorien (COMPDESCR) finden Sie bereits im Vektor docCats. Berechnen Sie auf dieser Basis die RANDMetrik zum Vergleich von Clusterings und interpretieren Sie diese soweit möglich.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnhtsa = dfnhtsa.sort_index(axis=1)\n",
    "dfnhtsa.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "best_topic_document = []\n",
    "\n",
    "for i in range(0, dfnhtsa.shape[0], 1):\n",
    "    topic_p = dfnhtsa.to_numpy()[i].tolist()\n",
    "    max_value = np.nanmax(topic_p)\n",
    "    max_index = topic_p.index(max_value)\n",
    "    \n",
    "    best_topic_document.append((i,max_index,max_value) )\n",
    "    \n",
    "max_p_df = pd.DataFrame(best_topic_document, columns=['Document', 'Topic', 'Probability (P)'])\n",
    "max_p_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAND-Metrik\n",
    "$Accuracy = \\frac{TP + TN}{TP+FP+FN+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docCats = relevantComplaints['COMPDESC'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_numpy(df):\n",
    "    df_topic = df['Topic']\n",
    "    df_doc = df['Document']\n",
    "    numpy_array_topic = df_topic.to_numpy()\n",
    "    numpy_array_doc = df_doc.to_numpy()\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    for t, d in np.nditer([numpy_array_topic, numpy_array_doc]):\n",
    "        for t2, d2 in np.nditer([numpy_array_topic, numpy_array_doc]):\n",
    "            c1 = docCats[int(d)]\n",
    "            c2 = docCats[int(d2)]\n",
    "            t1 = t\n",
    "            t2 = t2\n",
    "\n",
    "            if(c1 == c2):\n",
    "                if(t1 == t2):\n",
    "                    TP = TP + 1\n",
    "\n",
    "                if(t1 != t2):\n",
    "                    FP = FP + 1\n",
    "            else:\n",
    "                if(t1 == t2):\n",
    "                    FN = FN + 1\n",
    "\n",
    "                if(t1 != t2):\n",
    "                    TN = TN + 1\n",
    "\n",
    "    accuracy = (TP + TN) / (TP+FP+FN+TN)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy_numpy(max_p_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Aufgabe 3: Erstellen Sie eine Kreuztabellle, bei der eine Dimension die Topics und eine Dimension die Kategorien (COMPDESC) darstellen. In den Zellen soll gezählt werden, wie häufig im Corpus das Top-Topic eines Dokuments mit der tatsächlichen Kategorie korrespondiert. Nutzen Sie zur Visualisierung z.B. „clustermap“ aus der Python Bibliothek seaborn. Wie interpretieren Sie die Ergebnisse? Schauen Sie sich einzelne Dokumente als Repräsentanten/Beispiele interessanter Konstellationen in der Kreuztabelle an.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docCats_df = pd.DataFrame(docCats)\n",
    "docCats_df.columns = ['Cat']\n",
    "docCats_df_new = docCats_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p_df_new = max_p_df.reset_index()\n",
    "df_new = pd.merge(docCats_df_new, max_p_df_new, on='index')\n",
    "df_new\n",
    "df = df_new[['Cat', 'Document', 'Topic']]\n",
    "df.insert(3,'New',1)\n",
    "pivot_df = df.pivot_table(values='New', index='Cat', columns='Topic', aggfunc=np.sum)\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "ax = sns.heatmap(pivot_df, cmap='magma', linecolor='white', linewidth=1)\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('plots/heatmap_nhtsa.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.clustermap(pivot_df, cmap='coolwarm', standard_scale=1)\n",
    "plt.savefig('plots/clustermap_nhtsa.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Teil 3 - t3n Daten</span>\n",
    "## <span style=\"color:green\">Erstellen Sie ein bzw. ggf. mehrere Topicmodelle und übertragen Sie die Schritte bzw. Fragenstellungen aus Teil I und II soweit wie möglich. Dokumentieren Sie die Erkenntnisse etwas ausführlicher. Begründen Sie ausreichend, an welchen Stellen eine direkte Übertragung ggf. nicht möglich ist.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">gensim</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data ingestion and filtering\n",
    "t3n = pd.read_csv(\"data/t3n/T3N.csv\", keep_default_na=False)\n",
    "ta_t3n = pd.read_csv(\"data/t3n/T3NTEXTIND.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_t3n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n['CATEGORY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function \n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    nltk.download('stopwords')\n",
    "    german_stopwords = set(stopwords.words('german'))\n",
    "    data['TA_TOKEN'] = data['TA_TOKEN'].str.lower() \n",
    "    #display(data)\n",
    "    for index, row in data.iterrows():\n",
    "        if row.TA_TOKEN in german_stopwords:\n",
    "            data.drop(index, inplace=True)\n",
    "    display(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering and aggregation\n",
    "\n",
    "ta_t3n = ta_t3n[ta_t3n.TA_TYPE.isin(['noun', 'adjective']) ]\n",
    "ta_t3n.TA_TOKEN = ta_t3n.TA_TOKEN.str.lower()\n",
    "\n",
    "# remove stopwords\n",
    "ta_t3n = remove_stopwords(ta_t3n)\n",
    "#ta_t3n = ta_t3n[~ta_t3n.TA_TOKEN.isin(['car', 'vehicle']) ] #use as additional stop words\n",
    "t3n = t3n[t3n.CATEGORY.isin([ 'Mobilität', 'E-Commerce', 'Digitale Wirtschaft',\n",
    "       'Gadgets & Lifestyle', 'Marketing', 'Software & Infrastruktur',\n",
    "       'Digitale Gesellschaft', 'Entwicklung & Design', 'Startups',\n",
    "       'Karriere']) ]\n",
    "\n",
    "ta_t3n = ta_t3n.merge(t3n.loc[:,'ID'], on=['ID'], how='inner')\n",
    "tagrouped = ta_t3n.groupby('ID')['TA_TOKEN'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_t3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show category counts\n",
    "relevantT3n = t3n[t3n.ID.isin(tagrouped.index.tolist())].sort_values(by=['ID'])\n",
    "counter=collections.Counter(relevantT3n.CATEGORY)\n",
    "print(counter)\n",
    "print(\"Anzahl Dokumente: \" + str(len(relevantT3n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create gensim data structure\n",
    "t3ndic = corpora.Dictionary(tagrouped.tolist())\n",
    "t3ndic.filter_extremes(no_below=10, no_above=0.4)\n",
    "t3ndic.compactify()\n",
    "t3ncorpus = [t3ndic.doc2bow(text) for text in tagrouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#build topic model and create descriptive stuff\n",
    "t3nlda = LdaMulticore(t3ncorpus, num_topics=10, id2word = t3ndic, passes=20, alpha=0.0000001, random_state=1)\n",
    "t3nvis, dft3n = getModelResults(t3nlda, t3ncorpus, t3ndic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3nlda.print_topics(num_words=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3nvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">Lookup Table t3n</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    0: 'Software & Infrastruktur', \n",
    "    1: 'Entwicklung & Design',\n",
    "    2: 'Marketing',\n",
    "    3: 'Gadgets & Lifestyle',\n",
    "    4: 'E-Commerce', \n",
    "    5: 'Digitale Wirtschaft',\n",
    "    6: 'Startups', \n",
    "    7: 'Karriere', \n",
    "    8: 'Mobilität', \n",
    "    9: 'Marketing'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count a word in categories\n",
    "new_ta_t3n = ta_t3n[(ta_t3n['TA_TOKEN'] == 'apple')]\n",
    "df_ = pd.merge(new_ta_t3n, t3n, on='ID')\n",
    "sns.set(rc={'figure.figsize':(20,5)})\n",
    "sns.countplot(x='CATEGORY', data=df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">Softclustering in Hardclustering</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft3n = dft3n.sort_index(axis=1)\n",
    "dft3n.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "best_topic_document = []\n",
    "\n",
    "for i in range(0, dft3n.shape[0], 1):\n",
    "    topic_p = dft3n.to_numpy()[i].tolist()\n",
    "    max_value = np.nanmax(topic_p)\n",
    "    max_index = topic_p.index(max_value)\n",
    "    \n",
    "    best_topic_document.append((i,max_index,max_value) )\n",
    "    \n",
    "max_p_df = pd.DataFrame(best_topic_document, columns=['Document', 'Topic', 'Probability (P)'])\n",
    "max_p_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">RAND-Metrik</span>\n",
    "$Accuracy = \\frac{TP + TN}{TP+FP+FN+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docCats = relevantT3n['CATEGORY'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy_numpy(max_p_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">Cross Table</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docCats_df = pd.DataFrame(docCats)\n",
    "docCats_df.columns = ['Cat']\n",
    "docCats_df_new = docCats_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p_df_new = max_p_df.reset_index()\n",
    "df_new = pd.merge(docCats_df_new, max_p_df_new, on='index')\n",
    "df_new\n",
    "df = df_new[['Cat', 'Document', 'Topic']]\n",
    "df.insert(3,'New',1)\n",
    "pivot_df = df.pivot_table(values='New', index='Cat', columns='Topic', aggfunc=np.sum)\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "ax = sns.heatmap(pivot_df, cmap='magma', linecolor='white', linewidth=1)\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('plots/heatmap_t3n.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.fillna(inplace=True, value=0)\n",
    "ax = sns.clustermap(pivot_df, cmap='coolwarm', standard_scale=1)\n",
    "plt.savefig('plots/clustermap_t3n.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">sklearn</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_df = pd.read_csv('data/t3n/T3N.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t3n_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_german = pd.read_csv('data/german_stopwords.txt', header=None)[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words=stop_words_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cv.fit_transform(t3n_df['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, random_state=42, doc_topic_prior=1/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,topic in enumerate(lda.components_):\n",
    "    print(f'Die TOP-15 Wörter für das Thema #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = lda.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_df['Thema'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in max_p_df.iterrows():\n",
    "    for key in topics:\n",
    "        if(row['Topic'] == key):\n",
    "            row['Topic'] = topics[key]\n",
    "max_p_df           \n",
    "#metrics.adjusted_rand_score(docCats, max_p_df['Topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
