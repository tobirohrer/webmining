{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hdbcli import dbapi\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connection = dbapi.connect('34.254.159.229', 39015, 'SYSTEM', '')\n",
    "connection.isconnected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "t3n_text_index_df = pd.read_csv('data/T3NTEXTIND.csv')\n",
    "t3n_df = pd.read_csv('data/T3N.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3n_df = pd.read_csv('../praktikum2/t3n_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ermitteln Sie die häufigsten Folgen von Adjektiv-Nomen Bigrammen im Corpus (Hinweis: wenn Sie mit SQL arbeiten, können Sie einen non-equi auto-join verwenden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = 'select top 10 t1.TA_TOKEN, t2.TA_TOKEN, count(*) from \"SYSTEM\".\"$TA_CDESCRIND\" as t1, \"SYSTEM\".\"$TA_CDESCRIND\" as t2 where t1.cmplid=t2.cmplid and t1.TA_COUNTER=t2.TA_COUNTER-1 and t1.TA_SENTENCE=t2.TA_SENTENCE and t1.TA_TYPE=\\'adjective\\' and t2.TA_TYPE=\\'noun\\' group by t1.TA_TOKEN, t2.TA_TOKEN order by count(*) desc'\n",
    "cursor.execute(sql)\n",
    "adjective_noun = cursor.fetchall()\n",
    "adjective_noun_df = pd.DataFrame(adjective_noun)\n",
    "adjective_noun_df.columns = ['adjective', 'noun', 'count']\n",
    "adjective_noun_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ermitteln Sie die häufigsten Co-occurrences von Adjektiven innerhalb eines Satzes (Position egal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3n_adjectives = t3n_df[t3n_df['TA_TYPE']=='adjective']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_coocurrencies = t3n_adjectives.groupby(['ID','TA_SENTENCE'])['TA_TOKEN_LOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coocurrencies = {}\n",
    "\n",
    "#generate \"matrix\" with coocurrencies. we count the coocurrencies of each adverb per sentence. \n",
    "for name, group in grouped_coocurrencies:\n",
    "    for index_i, token_i in group.iteritems():\n",
    "        type(token_i)\n",
    "        if token_i not in coocurrencies:\n",
    "            coocurrencies[token_i] = {}\n",
    "            \n",
    "        for index_j, token_j in group.iteritems():\n",
    "            if token_j not in coocurrencies[token_i]:\n",
    "                coocurrencies[token_i][token_j] = 0\n",
    "            else:\n",
    "                coocurrencies[token_i][token_j]+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_coocurrencies = {}\n",
    "\n",
    "for coocurrency in coocurrencies:\n",
    "    coocurrencies_for_this_token = coocurrencies[coocurrency]\n",
    "    for coocurrency_for_this_token in coocurrencies_for_this_token:\n",
    "        if(coocurrency != coocurrency_for_this_token):\n",
    "            ordered_coocurrencies[coocurrency + '-' + coocurrency_for_this_token] = coocurrencies_for_this_token[coocurrency_for_this_token] \n",
    "            \n",
    "ordered_coocurrencies = {k: v for k, v in sorted(ordered_coocurrencies.items(), key=lambda item: item[1],reverse=True)}        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def drop_dublicates(ordered_coocurrencies):\n",
    "    for occurency in ordered_coocurrencies.copy():\n",
    "        split_occurency = occurency.split('-')\n",
    "        dublicate_occurency = split_occurency[1] + '-' + split_occurency[0]\n",
    "        if dublicate_occurency in ordered_coocurrencies and occurency in ordered_coocurrencies:\n",
    "            ordered_coocurrencies.pop(dublicate_occurency)\n",
    "    return ordered_coocurrencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_dublicates(ordered_coocurrencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Berechnen Sie die top-3 tf-idf (ntn nach der SMART Notation) Werte von Nomen im Corpus. Schauen Sie sich die entsprechenden Dokumente und die jeweiligen tf bzw. idf Wertkomponenten an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL-View MAX_FREQ_NOUN\n",
    "sql = 'create view MAX_FREQ_NOUN as select top 1 t1.TA_TOKEN as noun, count(*) as maxfreq from \"SYSTEM\".\"$TA_CDESCRIND\" as t1 where t1.TA_TYPE=\\'noun\\' group by t1.TA_TOKEN order by count(*) desc'\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SQL-View MAX_FREQ_NOUN\n",
    "sql_drop_view = 'drop view MAX_FREQ_NOUN'\n",
    "cursor.execute(sql_drop_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf\n",
    "sql = 'select top 3 t1.TA_TOKEN, (count(*)/t2.maxfreq) from \"SYSTEM\".\"$TA_CDESCRIND\" as t1, \"SYSTEM\".\"MAX_FREQ_NOUN\" as t2 where t1.TA_TYPE=\\'noun\\' group by t1.TA_TOKEN, t2.maxfreq order by count(*) desc'\n",
    "cursor.execute(sql)\n",
    "tf = cursor.fetchall()\n",
    "tf_df = pd.DataFrame(tf)\n",
    "tf_df.columns = ['noun', 'tf']\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL-View MAX_COUNT_NOUNS\n",
    "sql = 'create view MAX_COUNT_NOUNS as select top 3 t1.TA_TOKEN as noun from \"SYSTEM\".\"$TA_CDESCRIND\" as t1 where t1.TA_TYPE=\\'noun\\' group by t1.TA_TOKEN order by count(*) desc'\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL-View COUNT_DOCS\n",
    "sql = 'create view COUNT_DOCS as select count(DISTINCT cmplid) as n_total from \"SYSTEM\".\"$TA_CDESCRIND\"'\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL-View VALUES_IDF\n",
    "sql = 'create view VALUES_IDF as select t1.TA_TOKEN, count(DISTINCT cmplid) as count_docs_term, t3.n_total from \"SYSTEM\".\"$TA_CDESCRIND\" as t1, MAX_COUNT_NOUNS as t2, COUNT_DOCS as t3 where TA_TOKEN=t2.noun group by t1.TA_TOKEN, t3.n_total'\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SQL-Views\n",
    "sql_drop_view = 'drop view <view_name>'\n",
    "cursor.execute(sql_drop_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf\n",
    "sql = 'select TA_TOKEN, count_docs_term, n_total, LOG(10,(n_total/count_docs_term)) as idf from VALUES_IDF'\n",
    "cursor.execute(sql)\n",
    "idf = cursor.fetchall()\n",
    "idf_df = pd.DataFrame(idf)\n",
    "idf_df.columns = ['Token', 'count_docs_term', 'n_total', 'idf']\n",
    "idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Berücksichtigen Sie nur Adjektiv-Nomen Bigramme in Dokumenten mit der ID<1000000. Ermitteln Sie anhand des Chi2 Tests die 3 statistisch signifikantesten und die 3 am wenigsten signifikant zusammenhängenden Bigramme mit w1=* (beliebig) und w2=Tire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view\n",
    "# all adjective-noun bigrams with ID < 1000000 and w1=* and w2=Tire\n",
    "sql = 'create view adjective_noun_bigram as select t1.TA_TOKEN as adjective, t2.TA_TOKEN as noun, count(*) as  w1_and_w2 from \"SYSTEM\".\"$TA_CDESCRIND\" as t1, \"SYSTEM\".\"$TA_CDESCRIND\" as t2 where t1.cmplid=t2.cmplid and t1.TA_COUNTER=t2.TA_COUNTER-1 and t1.TA_SENTENCE=t2.TA_SENTENCE and t1.TA_TYPE=\\'adjective\\' and t2.TA_TYPE=\\'noun\\' and t1.cmplid<1000000 and t2.TA_TOKEN=\\'TIRE\\' group by t1.TA_TOKEN, t2.TA_TOKEN order by count(*) desc'\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view\n",
    "# all adjective-noun bigrams with ID < 1000000 and w1=* and w2=not_Tire\n",
    "sql = 'create view adjective_noun_bigram_not_tire as select t1.TA_TOKEN as adjective, t2.TA_TOKEN as noun, count(*) as  w1_and_not_w2 from \"SYSTEM\".\"$TA_CDESCRIND\" as t1, \"SYSTEM\".\"$TA_CDESCRIND\" as t2 where t1.cmplid=t2.cmplid and t1.TA_COUNTER=t2.TA_COUNTER-1 and t1.TA_SENTENCE=t2.TA_SENTENCE and t1.TA_TYPE=\\'adjective\\' and t2.TA_TYPE=\\'noun\\' and t1.cmplid<1000000 and t2.TA_TOKEN!=\\'TIRE\\' group by t1.TA_TOKEN, t2.TA_TOKEN order by count(*) desc'\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SQL-Views\n",
    "sql_drop_view = 'drop view <view_name>'\n",
    "cursor.execute(sql_drop_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "select adjective, noun, o11, o12, o21, o22 from \n",
    "(\n",
    "select t1.adjective, t1.noun, w1_and_w2 as o11, (sum_w1_and_w2-SUM(w1_and_w2)) as o12, t3.sum_w1_and_not_w2 as o21, (sum_NOT_w2 - t3.sum_w1_and_not_w2) as o22 from \n",
    "\"SYSTEM\".\"ADJECTIVE_NOUN_BIGRAM\" as t1, \n",
    "(select SUM(w1_and_w2) as sum_w1_and_w2 from \"SYSTEM\".\"ADJECTIVE_NOUN_BIGRAM\") as t2,\n",
    "(select adjective, SUM(w1_and_not_w2) as sum_w1_and_not_w2 from \"SYSTEM\".\"ADJECTIVE_NOUN_BIGRAM_NOT_TIRE\" group by adjective) as t3,\n",
    "(select SUM(w1_and_not_w2) as sum_NOT_w2 from \"SYSTEM\".\"ADJECTIVE_NOUN_BIGRAM_NOT_TIRE\" order by sum_NOT_w2 desc) as t4 where t3.adjective=t1.adjective group by t1.adjective, t1.noun, w1_and_w2, t2.sum_w1_and_w2, t3.sum_w1_and_not_w2, t4.sum_NOT_w2\n",
    ")\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_list = []\n",
    "header = [\"adjective\", \"noun\", \"o11\", \"o12\", \"o21\", \"o22\", \"chi_square\"];\n",
    "for item in data:\n",
    "    o11=item[2]\n",
    "    o12=item[3]\n",
    "    o21= item[4]\n",
    "    o22=item[5]\n",
    "    chi_square = ((102848 * (o11 * o22 - o12 * o21)**2) / ((o11 + o12) * (o11 + o21) * (o12 + o22) * (o21 + o22)))\n",
    "    values_list.append((item[0], item[1], item[2], item[3], item[4], item[5], chi_square))\n",
    "\n",
    "values_list.sort(key=lambda x: x[6], reverse=True);\n",
    "df = pd.DataFrame(values_list)\n",
    "df.columns = [header]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementieren Sie eine Ähnlichkeitsanfrage, die für einen gegebenen Anfragevektor (egal ob existierendes Dokument oder eigene Suchanfrage) die gerankten top n Dokumente anhand der Wortvektoren (nur Nomen ohne IDF-Gewichtung!) zurückgibt mit Hilfe von \n",
    "## a. Skalarprodukt (nnn nach SMART Notation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Kosinus Ähnlichkeit (nnc nach SMART Notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Welches sind die jeweils ähnlichsten Dokumente für Complaint 119408? Wie unterscheiden sich die Ergebnisse grundsätzlich?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testen Sie die Ähnlichkeitsanfragen für Ihren eigenen Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 2\n",
    "## 1. Implementieren oder wrappen Sie eine Python-Funktion (gerne mit Hilfe üblicher Bibliotheken wie sklearn.metrics.precision_recall_curve), die für Ihre vorbereiteten Anfragevektoren und die mit der Kosinus-Ähnlichkeit ermittelten Dokumente die Precision-Recall Plots zeichnet (Interpolation ist nicht nötig)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wie interpretieren Sie die Retrieval-Ergebnisse und die Plots (für Ihre eigenen Anfragen und mindestens für die Anfrage {'AIR': 2, 'BAG': 2, 'DEPLOYMENT':2, 'ACCIDENT':2} an NHTSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 3\n",
    "## 1. Führen Sie das Python Skript runMinHashExample aus. Versuchen Sie anhand des Tutorials und des Codes nachzuvollziehen, was in den einzelnen Schritten passiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Die Ergebnisse sind noch nicht ganz ideal (falls zufällig doch, probieren Sie andere Random Seeds aus. Überlegen Sie, welche (einfachen!) Freiheitsgrade Sie haben, um diese zu verbessern. Setzen Sie diese um und prüfen Sie jeweils, ob Sie eine Verbesserung erzielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementieren Sie als Alternative zum wortbasierten Shingling, ein zeichenbasiertes Shingling und vergleichen Sie die Ergebnisse systematisch für mehrere Fenstergrößen (und ggf. anderer variierender Freiheitsgrade)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
