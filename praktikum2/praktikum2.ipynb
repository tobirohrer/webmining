{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hdbcli import dbapi\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connection = dbapi.connect('52.214.219.42', 39015, 'SYSTEM', 'Glorp2018!')\n",
    "connection.isconnected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import t3n Data into HANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3n_data = pd.read_csv('../praktikum1/t3n_data_spider.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    sql = 'CREATE COLUMN TABLE \\\"SYSTEM\\\".\\\"T3N\\\" (\\\"ID\\\" VARCHAR(500),\\\"HEADING\\\" VARCHAR(2000),\\\"CATEGORY\\\" VARCHAR(2000),\\\"TEASER\\\" VARCHAR(5000),\\\"TEXT\\\" CLOB MEMORY THRESHOLD 1000,\\\"URL\\\" VARCHAR(2000), PRIMARY KEY (\\\"ID\\\"))'\n",
    "    cursor.execute(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_escaped_string(string):\n",
    "    return '\\'' + str(string) + '\\''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_data():\n",
    "    for index, row in t3n_data.iterrows():\n",
    "        sql_insert_datapoint = 'insert into \\\"SYSTEM\\\".\\\"T3N\\\" (ID, HEADING, CATEGORY, TEASER, TEXT, URL) VALUES (' + get_escaped_string(row['id']) + ', ' + get_escaped_string(row['heading']) +', ' + get_escaped_string(row['category']) + ',' + get_escaped_string(row['teaser']) + ', ' + get_escaped_string(row['text']) + ', ' + get_escaped_string(row['url'])+ ')'\n",
    "        print(sql_insert_datapoint)\n",
    "        #print(row['heading'])\n",
    "        cursor.execute(sql_insert_datapoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Textindex from HANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmpl_data = pd.read_csv('./_TA_CDESCRIND__201911271735.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution !! Takes ages ! \n",
    "cursor.execute('select * from \"$TA_CDESCRIND\"')\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns per Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmpl_nouns = cmpl_data[cmpl_data['TA_TYPE']==\"noun\"]\n",
    "nouns_per_doc = cmpl_nouns[\"CMPLID\"].value_counts()\n",
    "ax = sns.distplot(nouns_per_doc)\n",
    "ax.set(xlabel='#nouns per document')\n",
    "ax.set_title('Nouns per document')\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3n_data_tokenized = t3n_data\n",
    "t3n_data_tokenized['text'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['text'])), axis=1)\n",
    "t3n_data_tokenized['teaser'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['teaser'])), axis=1)\n",
    "t3n_data_tokenized['heading'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['heading'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in t3n_data.iterrows():\n",
    "    print(row['heading'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
