{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hdbcli import dbapi\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.cistem import Cistem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connection = dbapi.connect('34.255.100.176', 39015, 'SYSTEM', 'Glorp2018!')\n",
    "connection.isconnected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmpl_data = pd.read_csv('./_TA_CDESCRIND__201911271735.csv')\n",
    "t3n_data = pd.read_csv('./_TA_T3NTEXTIND__201912031756.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1 - Nouns per Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #Nouns per Document (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3n_nouns = t3n_data[t3n_data['TA_TYPE']==\"noun\"]\n",
    "nouns_per_doc = t3n_nouns[\"ID\"].value_counts()\n",
    "ax = sns.distplot(nouns_per_doc)\n",
    "ax.set(xlabel='#nouns per document')\n",
    "ax.set_title('Nouns per document')\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_nouns = cmpl_data[cmpl_data['TA_TYPE']==\"noun\"]\n",
    "cmpl_nouns.groupby('CMPLID')['TA_TOKEN'].value_counts()\n",
    "\n",
    "def get_tokencount_per_document(doc_id, df):\n",
    "    cmpl_nouns = df[df['TA_TYPE']==\"noun\"]\n",
    "    cmpl_id = cmpl_nouns[cmpl_nouns['CMPLID'] == doc_id]\n",
    "    return cmpl_id['TA_TOKEN'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL-View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SQL-View\n",
    "sql_drop_view = 'drop view COUNT_NOUNS'\n",
    "cursor.execute(sql_drop_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nouns per document\n",
    "sql = 'create view COUNT_NOUNS as select ID, TA_TOKEN, count(*) as COUNT from \"$TA_T3NTEXTIND\" where TA_TYPE=\\'noun\\' group by ID, TA_TOKEN'\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('select * from COUNT_NOUNS')\n",
    "nouns_list = cursor.fetchall()\n",
    "nouns_df = pd.DataFrame(nouns_list)\n",
    "nouns_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 Pandas Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Lexica before cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_lexica_size = cmpl_data['TA_TOKEN'].nunique()\n",
    "t3n_lexica_size = t3n_data['TA_TOKEN'].nunique()\n",
    "cmpl_normalized_lexica_size = cmpl_data['TA_NORMALIZED'].nunique()\n",
    "t3n_normalized_lexica_size = t3n_data['TA_NORMALIZED'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Lexica without punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unneeded_token_types(data):\n",
    "    TA_TYPES_TO_REMOVE = set({'punctuation', 'number'})\n",
    "    return data[~data['TA_TYPE'].isin(TA_TYPES_TO_REMOVE)]\n",
    "\n",
    "def remove_stopwords(data, language):\n",
    "    #nltk.download(language)\n",
    "    stopword_set = set(stopwords.words(language))\n",
    "    data['TA_TOKEN_LOW'] = data['TA_TOKEN'].map(lambda row: str(row).lower())\n",
    "    return data[~data['TA_TOKEN_LOW'].isin(stopword_set)]\n",
    "\n",
    "def remove_unique_tokens(data):\n",
    "    return data.groupby('TA_TOKEN').filter(lambda x: len(x) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_data_cleaned = remove_unneeded_token_types(cmpl_data)\n",
    "print(\"size of cmpl lexica after punctuation and number removal: \" + str(cmpl_data_cleaned['TA_TOKEN'].nunique()))\n",
    "cmpl_data_cleaned = remove_stopwords(cmpl_data_cleaned, 'english')\n",
    "print(\"size of cmpl lexica after stopword removal: \" + str(cmpl_data_cleaned['TA_TOKEN'].nunique()))\n",
    "cmpl_data_cleaned = remove_unique_tokens(cmpl_data_cleaned)\n",
    "print(\"size of cmpl lexica after removing unique words: \" + str(cmpl_data_cleaned['TA_TOKEN'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_data_cleaned = remove_unneeded_token_types(t3n_data)\n",
    "print(\"size of t3n lexica after punctuation and number removal: \" + str(t3n_data_cleaned['TA_TOKEN'].nunique()))\n",
    "t3n_data_cleaned = remove_stopwords(t3n_data_cleaned, 'german')\n",
    "print(\"removing german stopwords. #Tokens left: \" + str(t3n_data_cleaned['TA_TOKEN'].nunique()))\n",
    "t3n_data_cleaned = remove_unique_tokens(t3n_data_cleaned)\n",
    "print(\"removing unique tokens. #Tokens left: \" + str(t3n_data_cleaned['TA_TOKEN'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Lexica after Stemming  / Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Cistem() # German language stemmer\n",
    "t3n_data_cleaned['TA_STEMMED'] = t3n_data_cleaned['TA_NORMALIZED'].map(lambda token: stemmer.stem(str(token)))\n",
    "print(\"t3n lexica size before stemming: \" + str(t3n_data_cleaned['TA_TOKEN'].nunique()))\n",
    "print(\"t3n lexica size after stemming: \" + str(t3n_data_cleaned['TA_STEMMED'].nunique())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "cmpl_data_cleaned['TA_STEMMED'] = cmpl_data_cleaned['TA_NORMALIZED'].map(lambda token: lemmatizer.lemmatize(str(token)))\n",
    "print(\"cmpl lexica size before stemming: \" + str(cmpl_data_cleaned['TA_TOKEN'].nunique()))\n",
    "print(\"cmpl lexica size after stemming: \" + str(cmpl_data_cleaned['TA_STEMMED'].nunique())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Document / Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cmpl_doc_length = cmpl_data[\"CMPLID\"].value_counts().mean()\n",
    "mean_cmpl_sentence_length = cmpl_data.groupby(\"CMPLID\")[\"TA_SENTENCE\"].value_counts().mean()\n",
    "print(\"mean cmpl doc length: \" + str(mean_cmpl_doc_length))\n",
    "print(\"mean cmpl sentence length: \" + str(mean_cmpl_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_t3n_doc_length = t3n_data[\"ID\"].value_counts().mean()\n",
    "mean_t3n_sentence_length = t3n_data.groupby(\"ID\")[\"TA_SENTENCE\"].value_counts().mean()\n",
    "print(\"mean t3n doc length: \" + str(mean_t3n_doc_length))\n",
    "print(\"mean t3n sentence length: \" + str(mean_t3n_sentence_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_data_cleaned['FREQUENCY'] = t3n_data_cleaned['TA_TOKEN'].map(t3n_data_cleaned['TA_TOKEN'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent words\n",
    "sns.set(rc={'figure.figsize':(27,7)})\n",
    "most_occuring_tokens = t3n_data_cleaned.drop_duplicates(subset='TA_TOKEN').nlargest(20, 'FREQUENCY')\n",
    "sns.barplot(x=\"TA_TOKEN\", y=\"FREQUENCY\", data=most_occuring_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent verbs\n",
    "sns.set(rc={'figure.figsize':(27,7)})\n",
    "most_occuring_tokens = t3n_data_cleaned[t3n_data_cleaned['TA_TYPE'] == 'verb'].drop_duplicates(subset='TA_TOKEN').nlargest(20, 'FREQUENCY')\n",
    "sns.barplot(x=\"TA_TOKEN\", y=\"FREQUENCY\", data=most_occuring_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ambigious_tokens(df, id): \n",
    "    filtered = df[df['ID'] == id]\n",
    "    filtered = filtered.groupby(['TA_TOKEN', 'TA_TYPE']).size().reset_index(name='FREQ')\n",
    "    grouped = filtered.groupby(['TA_TOKEN'])\n",
    "    ambigious_tokens = grouped.filter(lambda x: len(x.groupby('TA_TYPE')) > 1)\n",
    "    ambigious_tokens = ambigious_tokens[['TA_TOKEN', 'TA_TYPE', 'FREQ']]\n",
    "    return ambigious_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ambigious_tokens(t3n_data_cleaned, '480af0e8-de34-4895-90df-337c38a60815')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3n_data_tokenized = t3n_data\n",
    "t3n_data_tokenized['text'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['text'])), axis=1)\n",
    "t3n_data_tokenized['teaser'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['teaser'])), axis=1)\n",
    "t3n_data_tokenized['heading'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['heading'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in t3n_data.iterrows():\n",
    "    print(row['heading'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
