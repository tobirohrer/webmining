{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hdbcli import dbapi\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.cistem import Cistem\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connection = dbapi.connect('52.18.25.218', 39015, 'SYSTEM', 'Glorp2018!')\n",
    "connection.isconnected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import t3n Data into HANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3n_data = pd.read_csv('../praktikum1/t3n_data_spider.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    sql = 'CREATE COLUMN TABLE \\\"SYSTEM\\\".\\\"T3N\\\" (\\\"ID\\\" VARCHAR(500),\\\"HEADING\\\" VARCHAR(2000),\\\"CATEGORY\\\" VARCHAR(2000),\\\"TEASER\\\" VARCHAR(5000),\\\"TEXT\\\" CLOB MEMORY THRESHOLD 1000,\\\"URL\\\" VARCHAR(2000), PRIMARY KEY (\\\"ID\\\"))'\n",
    "    cursor.execute(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_escaped_string(string):\n",
    "    return '\\'' + str(string) + '\\''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_data():\n",
    "    for index, row in t3n_data.iterrows():\n",
    "        sql_insert_datapoint = 'insert into \\\"SYSTEM\\\".\\\"T3N\\\" (ID, HEADING, CATEGORY, TEASER, TEXT, URL) VALUES (' + get_escaped_string(row['id']) + ', ' + get_escaped_string(row['heading']) +', ' + get_escaped_string(row['category']) + ',' + get_escaped_string(row['teaser']) + ', ' + get_escaped_string(row['text']) + ', ' + get_escaped_string(row['url'])+ ')'\n",
    "        print(sql_insert_datapoint)\n",
    "        #print(row['heading'])\n",
    "        cursor.execute(sql_insert_datapoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fulltext_index():\n",
    "    sql = 'CREATE FULLTEXT INDEX \\\"T3NTEXTIND\\\" ON \\\"SYSTEM\\\".\\\"T3N\\\" (\\\"TEXT\\\") CONFIGURATION \\'LINGANALYSIS_FULL\\' ASYNC LANGUAGE DETECTION (\\'de\\', \\'en\\') TEXT ANALYSIS ON'\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fulltext_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmpl_data = pd.read_csv('./_TA_CDESCRIND__201911271735.csv')\n",
    "t3n_data = pd.read_csv('./_TA_T3NTEXTIND__201911281048.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution !! Takes ages ! \n",
    "cursor.execute('select * from \"$TA_CDESCRIND\"')\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1 - Nouns per Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmpl_nouns = cmpl_data[cmpl_data['TA_TYPE']==\"noun\"]\n",
    "nouns_per_doc = cmpl_nouns[\"CMPLID\"].value_counts()\n",
    "ax = sns.distplot(nouns_per_doc)\n",
    "ax.set(xlabel='#nouns per document')\n",
    "ax.set_title('Nouns per document')\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_nouns = cmpl_data[cmpl_data['TA_TYPE']==\"noun\"]\n",
    "cmpl_nouns.groupby('CMPLID')['TA_TOKEN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokencount_per_document(doc_id, df):\n",
    "    cmpl_nouns = df[df['TA_TYPE']==\"noun\"]\n",
    "    cmpl_id = cmpl_nouns[cmpl_nouns['CMPLID'] == doc_id]\n",
    "    return cmpl_id['TA_TOKEN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tokencount_per_document(68, cmpl_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Lexica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_data['TA_TOKEN'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_data['TA_TOKEN'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_data['TA_NORMALIZED'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3n_data['TA_NORMALIZED'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Lexica without punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language(Enum):\n",
    "    GERMAN: 'german'\n",
    "    ENGLISH: 'english'    \n",
    "        \n",
    "def remove_unneeded_token_types(data):\n",
    "    TA_TYPES_TO_REMOVE = set({'punctuation', 'number'})\n",
    "    return data[~data['TA_TYPE'].isin(TA_TYPES_TO_REMOVE)]\n",
    "\n",
    "def remove_stopwords(data, language):\n",
    "    nltk.download(language)\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    data['TA_TOKEN_LOW'] = data['TA_TOKEN'].map(lambda row: str(row).lower())\n",
    "    return data[~data['TA_TOKEN_LOW'].isin(english_stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language(Enum):\n",
    "    GERMAN: 'german'\n",
    "    ENGLISH: 'english'    \n",
    "\n",
    "Language.GERMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_data_no_uneeded_tokens = remove_unneeded_token_types(cmpl_data)\n",
    "cmpl_data_no_uneeded_tokens['TA_TOKEN'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_data_no_stopwords_no_uneeded_tokens = remove_stopwords(cmpl_data_no_uneeded_tokens, Language.GERMAN)\n",
    "cmpl_data_no_stopwords_no_uneeded_tokens['TA_TOKEN'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Lexica after Stemming\n",
    "Das Anwenden von Stemming verringert die Größe des Lexikons von 71556 auf 63690, also um rund 11%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Cistem()\n",
    "cmpl_data['TA_STEMMED'] = cmpl_data['TA_NORMALIZED'].map(lambda token: stemmer.stem(str(token)))\n",
    "cmpl_data['TA_STEMMED'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Document / Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_doc_length = cmpl_data[\"CMPLID\"].value_counts().mean()\n",
    "mean_doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sentence_length = cmpl_data.groupby(\"CMPLID\")[\"TA_SENTENCE\"].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sentence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3n_data_tokenized = t3n_data\n",
    "t3n_data_tokenized['text'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['text'])), axis=1)\n",
    "t3n_data_tokenized['teaser'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['teaser'])), axis=1)\n",
    "t3n_data_tokenized['heading'] = t3n_data_tokenized.apply(lambda row: nltk.word_tokenize(str(row['heading'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in t3n_data.iterrows():\n",
    "    print(row['heading'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
