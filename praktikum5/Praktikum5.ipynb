{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWM Praktikum 5\n",
    "#### Wintersemester 2019/2020 (Prof. Dr. Markus Döhring, Dr. Steffen Schnitzer, Nina Krüger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "# NUR bei RosettaHub ändern: Cell>CellType>Code\n",
    "for K in $(apt-key list | grep expired | cut -d'/' -f2 | cut -d' ' -f1); do sudo apt-key adv --recv-keys --keyserver keys.gnupg.net $K; done\n",
    "sudo cat > /etc/apt/sources.list << EOF\n",
    "deb http://archive.ubuntu.com/ubuntu xenial main restricted universe multiverse\n",
    "deb-src http://archive.ubuntu.com/ubuntu xenial main restricted universe multiverse\n",
    "deb http://archive.ubuntu.com/ubuntu xenial-updates main restricted universe multiverse\n",
    "deb-src http://archive.ubuntu.com/ubuntu xenial-updates main restricted universe multiverse\n",
    "deb http://archive.ubuntu.com/ubuntu xenial-backports main restricted universe multiverse\n",
    "deb-src http://archive.ubuntu.com/ubuntu xenial-backports main restricted universe multiverse\n",
    "deb http://archive.ubuntu.com/ubuntu xenial-security main restricted universe multiverse\n",
    "deb-src http://archive.ubuntu.com/ubuntu xenial-security main restricted universe multiverse\n",
    "#deb http://archive.ubuntu.com/ubuntu xenial-proposed restricted main universe multiverse\n",
    "#deb-src http://archive.ubuntu.com/ubuntu xenial-proposed restricted main universe multiverse\n",
    "deb http://archive.canonical.com/ubuntu xenial partner\n",
    "deb-src http://archive.canonical.com/ubuntu xenial partner\n",
    "EOF\n",
    "sudo apt-get -y install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 1==0: #ändern in 1==1, falls auf RosettaHub ausgeführt\n",
    "    !{sys.executable} -m pip install --upgrade tensorflow-gpu\n",
    "    !{sys.executable} -m pip install  nltk\n",
    "    !{sys.executable} -m pip install  sklearn\n",
    "    !{sys.executable} -m pip install pydotplus\n",
    "    !{sys.executable} -m pip install  graphviz\n",
    "    !{sys.executable} -m pip install  --upgrade keras\n",
    "    !{sys.executable} -m pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 1==0: #ändern in 1==1, falls auf TWM ausgeführt\n",
    "    !{sys.executable} -m pip install --ignore-installed --upgrade tensorflow-gpu\n",
    "    !{sys.executable} -m pip install --ignore-installed nltk\n",
    "    !{sys.executable} -m pip install --ignore-installed sklearn\n",
    "    !{sys.executable} -m pip install --ignore-installed pydotplus\n",
    "    !{sys.executable} -m pip install --ignore-installed graphviz\n",
    "    !{sys.executable} -m pip install --ignore-installed --upgrade keras\n",
    "    !{sys.executable} -m pip install --ignore-installed --upgrade tensorflow\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    !{sys.executable} -m pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports und keras/tensorflow Einstellungen für Reproduzierbarkeit\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(1)\n",
    "rn.seed(2)\n",
    "tf.set_random_seed(3)\n",
    "\n",
    "import keras\n",
    "keras.__version__\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image, display  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import json\n",
    "import numpy\n",
    "import nltk\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wir setzen eine globalen Parameter für die maximale Länge von Reviews.\n",
    "#Alle Reviews, die länger als review_length Wörter sind, werden vor der Modellbildung entsprechend gekürzt \n",
    "review_length = 500\n",
    "\n",
    "#ein weiterer globaler Parameter gibt an, wie groß das Dictionnary sein darf.\n",
    "#Es werden grundsätzlich die nwords am häufigsten im Corpus vorkommenden Wörter verwendet. \n",
    "nwords = 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinweis: ein Großteil dieses Notebooks basiert auf dem Buch \"Deep Learning with Python\" (Chollet 2017), insbesondere Kapitel 6, und entsprechend bereitgestelltem Code: https://github.com/fchollet/deep-learning-with-python-notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMDB Movie Reviews - Explorative Datenanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Praktikum 5 arbeiten wir mit dem IMDB Datensatz, der durch Keras bereitgestellt wird. Dieser enthält insgesamt 50.000 stark positiv/negativ geprägte Reviews. Der folgende Python Code liefert 25.000 Reviews (X\\_ Arrays) mit entsprechenden Labels (Y\\_ Arrays) für das Training und 25.000 Reviews für das Testen eines Modells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=nwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die von Keras bereitgestellten Datenstrukturen bestehen aus Arrays von Ganzzahlen, die Einträge in einem Lexikon darstellen. Um uns Texte anschauen (oder ggf. textuell weiterverarbeiten) zu können, müssen wir diese \"zurückübersetzen\", d.h. die Ganzzahlen wieder in Wörter transformieren. Weiterhin kürzen wir die Reviews auf eine in den globelen Parmetern gesetzte Länge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index ist ein dictionnary, das Wörter auf eine Ganzzahl abbildet\n",
    "word_index = imdb.get_word_index()\n",
    "# wir kehren den Word Index um und bilden nun Ganzzahlen auf Wörter ab\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Wir dekodieren die Reviews. Achtung: Die Indizes sind um 3 verschoben\n",
    "# weil 0, 1 and 2 reservierte Indizes für \"padding\", \"start of sequence\" und \"unknown\" sind\n",
    "X_train = [sequence[0:min(review_length,len(sequence))] for sequence in X_train]\n",
    "X_test = [sequence[0:min(review_length,len(sequence))] for sequence in X_test]\n",
    "X_train = [\n",
    "    ' '.join(\n",
    "        reverse_word_index.get(i - 3, '?')\n",
    "        for i in X_train[i]\n",
    "        if i < len(reverse_word_index)\n",
    "    ) for i in range(len(X_train))\n",
    "]\n",
    "\n",
    "X_test = [\n",
    "    ' '.join(\n",
    "        reverse_word_index.get(i - 3, '?')\n",
    "        for i in X_test[i]\n",
    "        if i < len(reverse_word_index)\n",
    "    ) for i in range(len(X_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1: Schauen Sie sich einige Reviews und deren Labels an\n",
    "Ist ein Datensatz mit 1 gelabelt, dann steht dies fuer einen eher positives Review. Die 0 als Label steht fuer ein eher negatives Review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#IHR CODE HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2: Identifizieren Sie die relevantesten Features (Wörter) anhand des Chi²-Wertes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IHR CODE HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3: Prüfen und begründen Sie, ob „accuracy“ ein sinnvolles Gütemaß für einen Klassifikator auf diesem Datensatz ist.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IHR CODE+BEGRÜNDUNG HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Learning\n",
    "### 2.1. RNN mit Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden lernen wir ein RNN mit Embeddings Layer über 10 Iterationen (Epochen). Für jede Epoche wird die Performance (accuracy) für die Trainings- und die Validierungsdaten (20%, die vom ursprünglichen Trainingsdatensatz nochmal beiseite gelegt wurdden) ausgegeben. Am Ende werden die entsprechenden Werte im Zeitverlauf graphisch dargestellt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=nwords)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=review_length)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=review_length)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras.layers import Dense\n",
    "import keras \n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir='/home/student/PycharmProjects/test/logs/', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model = Sequential()\n",
    "model.add(Embedding(nwords, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2#,\n",
    "                     #callbacks=[tbCallBack]\n",
    "                   )            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plothist(hist):\n",
    "    \n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plothist(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 4: Wie interpretieren Sie die Performance-Kurven zum RNN bzw. was fällt Ihnen auf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHRE INTERPRETATION HIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basierend auf der Train-Validation Performance entscheiden wir, unser Modell auf den gesamten Trainingsdaten über 4 Epochen zu trainieren. Dann geben wir die Performance auf den Testdaten aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "\n",
    "model.fit(input_train, y_train, epochs=2, batch_size=128)\n",
    "results = model.evaluate(input_test, y_test)\n",
    "results[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 5: Schauen Sie sich einige der \"drastischsten\" false positives und false negatives an (hohe Score und Label=1 oder niedrige Score und Label=0). Können Sie erahnen, was das Modell ggf. verwirrt hat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#IHR CODE+BEGRÜNDUNG HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN mit Glove 50 Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun trainieren wir ein RNN mit der selben Architektur wie vorher, nur dass wir anstelle eines parallel trainierten 32-dimensionalen Embeddings ein 50-dimensionales, vortrainiertes Word-Embedding verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "glove6B = 'glove.6B.50d.txt'\n",
    "\n",
    "if os.path.isfile(glove6B):\n",
    "    print('Glove tokens file already exists. No need to download it.')\n",
    "    f = open(glove6B)\n",
    "else:\n",
    "    print('Glove tokens file does not exists. Download can take some time.')\n",
    "    url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    resp = requests.get(url)\n",
    "    zipfile = ZipFile(BytesIO(resp.content))\n",
    "    f = zipfile.open(glove6B)\n",
    "    #saving it for the next run\n",
    "    zipfile.extract(glove6B)\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "from keras.datasets import imdb\n",
    "embedding_dim = 50\n",
    "word_index = imdb.get_word_index()\n",
    "embedding_matrix = np.zeros((nwords, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < nwords:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Embedding(nwords, 50))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2#,\n",
    "                     #callbacks=[tbCallBack]\n",
    "                   )            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plothist(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "model.fit(input_train, y_train, epochs=3, batch_size=128)\n",
    "results = model.evaluate(input_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 6: Optional (nur wenn Sie gut in der Zeit liegen und fit in der Materie sind!): Wie interpretieren Sie die Performance-Kurven und Ergebnisse auf den Testdaten zum RNN mit vorgelernten Glove-Embedding bzw. was fällt Ihnen auf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHRE INTERPRETATION HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun trainieren wir anstatt eines Modells mit normalen RNN Zellen ein Modell mit LSTM Zellen. Ansonsten bleiben alle anderen Rahmenbedingungen identisch zum Modell unter 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nwords, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plothist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "model.fit(input_train, y_train, epochs=4, batch_size=128)\n",
    "results = model.evaluate(input_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 7: Wie interpretieren Sie die Performance-Kurven und Ergebnisse auf den Testdaten zum LSTM bzw. was fällt Ihnen auf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHRE INTERPRETATION HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Not So Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden verwenden wir sehr einfache Klassifikationsmodelle: Eine logistische Regression (siehe ggf. Wikipedia o.ä.) und einen Decision Tree aufbauend auf TF-IDF gewichteten Unigrammen+Bigrammen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('log', LogisticRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))\n",
    "print((y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Features sortiert nach ihren Koeffizienten innerhalb des LogReg Modells\n",
    "np.array(model.named_steps['tfidf'].get_feature_names())[np.argsort(model.named_steps['log'].coef_[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('tree', DecisionTreeClassifier(criterion='gini', max_depth=4))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))\n",
    "print((y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "\n",
    "export_graphviz(model.named_steps['tree'], out_file=dot_data, filled=True, rounded=True, special_characters=True)\n",
    "\n",
    "splitArgs = model.named_steps['tree'].tree_.feature.tolist()\n",
    "\n",
    "dot_data = dot_data.getvalue()\n",
    "\n",
    "print('Das sind die Splitargumente des DecisionTrees:')\n",
    "\n",
    "for x in ([arg for arg in splitArgs if arg != -2 ]):\n",
    "    rep = model.named_steps['tfidf'].get_feature_names()[x] \n",
    "    dot_data = dot_data.replace('X<SUB>'+str(x)+'</SUB>', str(rep))\n",
    "    print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# Falls Sie https://www.graphviz.org/ installiert haben, koennen Sie den Decision Tree auch plotten\n",
    "# In der twm VM koennen Sie graphviz mit: sudo apt-get -y install graphviz\n",
    "# installieren\n",
    "try:\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    display(Image(graph.create_png()))\n",
    "except:\n",
    "    print('graphviz ist nicht installiert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 8: Wie interpretieren Sie die Ergebnisse der \"einfacheren\" Klassifikationsmodelle auf den Testdaten bzw. was fällt Ihnen auf? Inwiefern deckt sich die Wichtigkeit der Features mit der, die Sie in Aufgabe (2) ermittelt haben?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHRE INTERPRETATION HIER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
